{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a68fa1d-a314-4fcd-a08a-cb02c800a426",
   "metadata": {},
   "source": [
    "## Python Windows 64bit (x86_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59ba2cd4-4b49-472e-b8ff-8b4d21e4ae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD64\n",
      "Windows\n",
      "ARMv8 (64-bit) Family 8 Model 1 Revision 201, Qualcomm Technologies Inc\n"
     ]
    }
   ],
   "source": [
    "# Let's first ensure we're using the correct platform that's necessary for inference\n",
    "# To quantize model we need AMD64 (x86_64): 64-bit Python on 64-bit x86 architecture\n",
    "\n",
    "import platform\n",
    "\n",
    "arch = platform.machine()\n",
    "sys = platform.system()\n",
    "processor = platform.processor()\n",
    "print(arch)\n",
    "print(sys)\n",
    "print(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c026000c-0f75-41c4-87d2-394befc688c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works using Python x86_64\n",
    "# Per directions use Python Windows 64 bit (x86_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39c717fe-27ff-4c2d-b103-9e4547e4d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from onnxruntime.quantization import CalibrationDataReader\n",
    "from PIL import Image\n",
    "from typing import List,Tuple\n",
    "\n",
    "random.seed(41)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820eb39-c82a-4261-a0b3-6b449e6da36f",
   "metadata": {},
   "source": [
    "https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/resnet50_data_reader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0acfe05-20ac-4797-af05-5f19fe2ec78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader(CalibrationDataReader):\n",
    "    def __init__(self, model_path:str, dataset_path:str, img_max: int=10, verbose=False) -> None:\n",
    "        self.enum_data = None\n",
    "\n",
    "        self.session = onnxruntime.InferenceSession(model_path, provider=[\"CPUExecutionProvider\"])\n",
    "        self.expected_inputs = self.session.get_inputs()[0]\n",
    "        \n",
    "        #Need to use expected name and shape for model quantization\n",
    "        self.expected_name = self.expected_inputs.name\n",
    "        self.expected_shape = self.expected_inputs.shape\n",
    "        self.data_list = []\n",
    "\n",
    "        self.transformer = v2.Compose([\n",
    "            v2.Resize(size=(self.expected_shape[2], \n",
    "                            self.expected_shape[3]),\n",
    "                            interpolation=InterpolationMode.BICUBIC),\n",
    "            v2.PILToTensor(),\n",
    "            v2.ConvertImageDtype(torch.float32),\n",
    "        ])\n",
    "        self.re_mapper = re.compile(r\"^C:\\\\.+?\\\\Human_Action_Recognition\\\\test\\\\(?P<image>Image_\\d{1,4}).jpg\")\n",
    "                \n",
    "        img_files = [os.path.join(dataset_path,file) for file in os.listdir(dataset_path) if file.endswith(\".jpg\")]\n",
    "        random.shuffle(img_files)\n",
    "        for file in img_files[:img_max]:\n",
    "            img_pil = Image.open(file)\n",
    "            img_transformed = np.expand_dims(self._image_transformer(frame=img_pil), axis=0)\n",
    "            if verbose:\n",
    "                print(f\"OG Image Shape: {np.array(img_pil).shape}\")\n",
    "                print(f\"Transformed Shape: {img_transformed.shape}\")\n",
    "                \n",
    "\n",
    "            self.data_list.append(img_transformed)\n",
    "            \n",
    "        self.data_size = len(self.data_list)    \n",
    "\n",
    "    def _image_transformer(self, frame: Image) -> Image:\n",
    "        transformed_frame = self.transformer(frame)\n",
    "        transformed_frame_np = np.array(transformed_frame)\n",
    "        return transformed_frame_np\n",
    "        \n",
    "        \n",
    "    def get_next(self):\n",
    "        # Updated this from resnet quantization example\n",
    "        # https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/resnet50_data_reader.py\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(\n",
    "                [{self.expected_name: img_data} for img_data in self.data_list]\n",
    "            )\n",
    "        return next(self.enum_data, None)\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = None\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6eb5dc9-6a7f-4aa1-86c3-74e92da4b9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\DFS\\\\Desktop\\\\datasets\\\\Human_Action_Recognition\\\\test'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = os.path.join(\"C:\\\\\",\"Users\",\"DFS\",\"Desktop\",\"datasets\",\"Human_Action_Recognition\",\"test\")\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e1077-ae0f-4f5b-87aa-53049315c244",
   "metadata": {},
   "source": [
    "https://onnxruntime.ai/docs/execution-providers/QNN-ExecutionProvider.html#running-a-model-with-qnn-eps-htp-backend-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df4cc6b5-98e7-4958-8f9a-a434850453ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!powershell python -m onnxruntime.quantization.preprocess --input \"../models/hrnet_pose.onnx\" --output \"../models/hrnet_quantized_preprocessed_0129.onnx\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fe5abc7-1ec1-483f-b569-c504483fbbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'op_types_to_quantize': ['Sub', 'Add', 'Relu', 'Div', 'Conv', 'Resize'],\n",
       " 'per_channel': False,\n",
       " 'reduce_range': False,\n",
       " 'weight_type': <QuantType.QUInt8: 1>,\n",
       " 'activation_type': <QuantType.QUInt16: 4>,\n",
       " 'nodes_to_quantize': [],\n",
       " 'nodes_to_exclude': [],\n",
       " 'use_external_data_format': False,\n",
       " 'calibration_data_reader': <__main__.DataReader at 0x2c992225650>,\n",
       " 'calibrate_method': <CalibrationMethod.MinMax: 0>,\n",
       " 'quant_format': <QuantFormat.QDQ: 1>,\n",
       " 'extra_options': {'MinimumRealRange': 0.0001,\n",
       "  'DedicatedQDQPair': False,\n",
       "  'QDQKeepRemovableActivations': False,\n",
       "  'TensorQuantOverrides': {},\n",
       "  'ActivationSymmetric': False,\n",
       "  'WeightSymmetric': False,\n",
       "  'CalibStridedMinMax': None,\n",
       "  'UseQDQContribOps': True}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from onnxruntime.quantization import QuantType, quantize, preprocess\n",
    "from onnxruntime.quantization.execution_providers.qnn import get_qnn_qdq_config, qnn_preprocess_model\n",
    "\n",
    "base_path = os.path.join(\"..\",\"models\")\n",
    "model_path = os.path.join(base_path,\"hrnet_pose.onnx\")\n",
    "output_model_path = os.path.join(base_path,\"hrnet_quantized_0129.onnx\")\n",
    "\n",
    "#Pre-process original float32 model\n",
    "pre_process_model_path = os.path.join(base_path,\"hrnet_quantized_preprocessed_0129.onnx\")\n",
    "\n",
    "#Be sure this preprocess is actually preprocessing the onnx model\n",
    "model_diff = qnn_preprocess_model(model_path, pre_process_model_path)\n",
    "model_to_quantize = pre_process_model_path #if model_diff else model_path\n",
    "\n",
    "iDataReader = DataReader(model_path=model_path, dataset_path=dataset_path, verbose=False)\n",
    "\n",
    "qnn_config = get_qnn_qdq_config(model_to_quantize,  #\n",
    "                                iDataReader,\n",
    "                                activation_type=QuantType.QUInt16,\n",
    "                                weight_type=QuantType.QUInt8)\n",
    "qnn_config.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c333d17-58b9-4535-9331-7d7d287bd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize(model_to_quantize, output_model_path, qnn_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2d584-7d8c-4f07-b53e-acfa341127cf",
   "metadata": {},
   "source": [
    "1. Download an x64 version of Python for windows (Windows x86-64) (not ARM64)\n",
    "2. Install x64 Version\n",
    "3. Set up windows alias for x64\n",
    "4. Create virtual environment utilizing the x64 python\n",
    "5. Activate virtual environment and rerun this notebook\n",
    "   \n",
    "**Generating a quantized model (x64 only)**\n",
    "    \n",
    "    The ONNX Runtime python package provides utilities for quantizing ONNX models via the onnxruntime.quantization import. The quantization utilities are currently only supported on x86_64 due to issues installing the onnx package on ARM64. Therefore, it is recommended to either use an x64 machine to quantize models or, alternatively, use a separate x64 python installation on Windows ARM64 machines.\n",
    "    \n",
    "    Install the ONNX Runtime x64 python package. (please note, you must use x64 package for quantizing the model. use the arm64 package for inferencing and utilizing the HTP/NPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d421da-b5ff-41fe-b05d-9b0353d9e51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_win64)",
   "language": "python",
   "name": "env_win64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
