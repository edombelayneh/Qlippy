{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede68370-32c3-4009-bc2b-2502bce0a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c152d4-00b0-4946-8202-698f8bf25322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check to ensure we are in the proper environment, remember we need to be in pure arm64\n",
    "import platform\n",
    "\n",
    "arch = platform.machine()\n",
    "sys = platform.system()\n",
    "processor = platform.processor()\n",
    "print(f\"{arch}\\n{sys}\\n{processor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d0e3f-d3ef-433c-a134-a9c7c3ada10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary tools that we need\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c2eb2-0e89-42cd-b112-7ea0b2a0faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the root directory as a reference\n",
    "root_dir = Path.cwd().parent.parent\n",
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b0ac5-3a4f-46c5-b14a-f5bafed7b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the path to onnxruntime therefore we can grab hexagon driver\n",
    "onnx_root = Path(ort.__file__).parent\n",
    "onnx_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028612de-d6a6-4d6a-a30a-5f2f8c978548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory where all .onnx dependencies are located\n",
    "model_subdirectory = \"qnn-deepseek-r1-distill-qwen-7b\"\n",
    "\n",
    "# The embeddings model is entry point, use netron to visualize\n",
    "model_name = \"deepseek_r1_7b_embeddings_quant_v1.0.onnx\"\n",
    "\n",
    "# This graph is used to process initial prompt, we can pass up to 64 tokens\n",
    "context_model = \"deepseek_r1_7b_ctx_v1.0.onnx_ctx.onnx\"\n",
    "\n",
    "# This graph is used to perform next word inference after the initial prompt\n",
    "context_model_iter = \"deepseek_r1_7b_iter_v1.0.onnx_ctx.onnx\"\n",
    "\n",
    "# This graph allows us to take hidden states and return logits\n",
    "head_model = \"deepseek_r1_7b_head_quant_v1.0.onnx\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer_json = \"tokenizer.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a4721-e62c-48c2-962b-650738f9d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solidifying all paths\n",
    "\n",
    "model_path = root_dir/\"models\"/model_subdirectory/model_name\n",
    "ctx_path = root_dir/\"models\"/model_subdirectory/context_model\n",
    "ctx_path_itr = root_dir/\"models\"/model_subdirectory/context_model_iter\n",
    "head_path = root_dir/\"models\"/model_subdirectory/head_model\n",
    "tokenizer_path = root_dir/\"models\"/model_subdirectory/tokenizer_json\n",
    "hexagon_driver = onnx_root/\"capi\"/\"QnnHtp.dll\" #\"C:/Users/DFS/.vscode/extensions/ms-windows-ai-studio.windows-ai-studio-0.14.4-win32-arm64/bin/QnnHTP.dll\"#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a804b38-f006-468d-9052-f710f90fc90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f40fa0-584f-4006-83c3-e1c8d4bd44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hexagon_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383718b-aa81-4897-9372-9c26519f353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_options = ort.SessionOptions()\n",
    "\n",
    "qnn_provider_options = {\n",
    "    # Path to the backend driver \"Hexagon\"\n",
    "    \"backend_path\": hexagon_driver,\n",
    "    # https://onnxruntime.ai/docs/execution-providers/QNN-ExecutionProvider.html#configuration-options\n",
    "    \"htp_performance_mode\": \"burst\",\n",
    "    \"soc_model\": \"60\",\n",
    "    # \"enable_htp_context_cache\": \"0\",\n",
    "    \"profiling_level\": \"detailed\",\n",
    "    \"profiling_file_path\": root_dir/\"models\"/model_subdirectory/\"profiling_deepseek_7b.csv\",\n",
    "    # Enabling graph optimization causes problems, need to look into this\n",
    "    \"htp_graph_finalization_optimization_mode\": \"3\",\n",
    "    \"qnn_context_priority\": \"high\"\n",
    "}\n",
    "\n",
    "# Creating an inference session for the embedding graph\n",
    "embedding_session = ort.InferenceSession(model_path,\n",
    "                                providers= [(\"QNNExecutionProvider\",qnn_provider_options)],\n",
    "                               sess_options= session_options\n",
    "                              )\n",
    "\n",
    "# Creating an inference session for the single prediction context graph (iter_ctx)\n",
    "ctx_itr_session = ort.InferenceSession(ctx_path_itr,\n",
    "                                         providers=[(\"QNNExecutionProvider\",qnn_provider_options)],\n",
    "                                         sess_options= session_options\n",
    "                                      )\n",
    "\n",
    "# Creating an inference session for the initial context graph\n",
    "ctx_session = ort.InferenceSession(ctx_path,\n",
    "                                    providers=[(\"QNNExecutionProvider\",qnn_provider_options)],\n",
    "                                    sess_options= session_options\n",
    "                                        )\n",
    "\n",
    "# Creating an inference session for the head session which will provide logits from hidden states\n",
    "head_session = ort.InferenceSession(head_path,\n",
    "                                providers= [(\"QNNExecutionProvider\",qnn_provider_options)],\n",
    "                               sess_options= session_options\n",
    "                              )\n",
    "\n",
    "embedding_session.get_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff6b9fe-5d0f-48fd-8e9c-25b8d8b246fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = embedding_session.get_inputs()\n",
    "outputs = embedding_session.get_outputs()\n",
    "input_0 = inputs[0]\n",
    "output_0 = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ccb3e9-2ec0-42b9-8495-f295483ead0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected Input Shape: {input_0.shape}\")\n",
    "print(f\"Expected Input Type: {input_0.type}\")\n",
    "print(f\"Expected Input Name: {input_0.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf9daa-63f6-41c4-9f6e-6351131aa27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected Output Shape: {output_0.shape}\")\n",
    "print(f\"Expected Output Type: {output_0.type}\")\n",
    "print(f\"Expected Output Name: {output_0.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eed8ad-dd0b-449c-8fbe-269ce71927d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ctx = ctx_session.get_inputs()\n",
    "outputs_ctx = ctx_session.get_outputs()\n",
    "input_0_ctx = inputs_ctx[0]\n",
    "output_0_ctx = outputs_ctx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44648ec-b4f0-4489-9825-49158dc474d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected Input Shape: {input_0_ctx.shape}\")\n",
    "print(f\"Expected Input Type: {input_0_ctx.type}\")\n",
    "print(f\"Expected Input Name: {input_0_ctx.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79374398-0764-4833-b5e6-1a6b0d6299dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected Output Shape: {output_0_ctx.shape}\")\n",
    "print(f\"Expected Output Type: {output_0_ctx.type}\")\n",
    "print(f\"Expected Output Name: {output_0_ctx.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac8b94-d472-4e07-b2a2-e8355abd6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ctx_itr = ctx_itr_session.get_inputs()\n",
    "outputs_ctx_itr = ctx_itr_session.get_outputs()\n",
    "input_0_ctx_itr = inputs_ctx_itr[0]\n",
    "output_0_ctx_itr = outputs_ctx_itr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753cb97-f95e-415b-b132-9fa3118dd22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected Input Shape: {input_0_ctx_itr.shape}\")\n",
    "print(f\"Expected Input Type: {input_0_ctx_itr.type}\")\n",
    "print(f\"Expected Input Name: {input_0_ctx_itr.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9327a9-4c25-41d2-a95b-4eb707d13810",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected Output Shape: {output_0_ctx_itr.shape}\")\n",
    "print(f\"Expected Output Type: {output_0_ctx_itr.type}\")\n",
    "print(f\"Expected Output Name: {output_0_ctx_itr.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4ce68-d015-4d4d-9c5a-78fd5420ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_head = head_session.get_inputs()\n",
    "outputs_head = head_session.get_outputs()\n",
    "input_0_head = inputs_head[0]\n",
    "output_0_head = outputs_head[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24753436-4610-4f19-9402-891c3f89e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected Input Name: {input_0_head.name}\")\n",
    "print(f\"Expected Input Shape: {input_0_head.shape}\")\n",
    "print(f\"Expected Input Type: {input_0_head.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6633be-06d6-4c1e-813b-f215088fc2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected Output Name: {output_0_head.name}\")\n",
    "print(f\"Expected Output Shape: {output_0_head.shape}\")\n",
    "print(f\"Expected Output Type: {output_0_head.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79c33e-967a-4652-af39-e6afa06a0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in tokenizer using tokenizer path above\n",
    "tokenizer = Tokenizer.from_file(str(tokenizer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5359713-ee6e-430c-9e29-a06227bff28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An initial query\n",
    "init_query = \"<｜User｜>\\nYou are a poet and a scholar. Give me a sonnet explaining the importance of running large language models locally\\n<｜Assistant｜><think>\\n\"\n",
    "encoding = tokenizer.encode(init_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e20101-0a5b-490d-bb4a-a25e02617b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token IDs:\", encoding.ids)\n",
    "print(\"Tokens:\", encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad39ea5-7780-428b-b29c-8b1cceeb5138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = encoding.ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b5ac0-fd4e-4dae-a8ab-a1b7acb54410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = np.array([input_ids], dtype=np.int64)\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4dd5e7-9e36-481f-8a6f-332aae0c1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run embedding session first\n",
    "embedding_output = embedding_session.run(None, {\"input_ids\":input_ids})[0]\n",
    "print(\"(batch, sequence length, embedding dimension)\")\n",
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a6da0-a753-4f53-ae38-0775a3d27ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing inputs for prompt\n",
    "\n",
    "# Number of input sequences processed simultaneously\n",
    "batch_size = 1\n",
    "\n",
    "# Current sequence length for initial prompt (number of tokens in current sequence)\n",
    "seq_len = embedding_output.shape[1]\n",
    "\n",
    "# Dimensionality of each token embedding vector\n",
    "hidden_size = embedding_output.shape[2]\n",
    "\n",
    "# Number of attention heads in each transformer layer\n",
    "num_heads = 28\n",
    "\n",
    "# Size of each attention head\n",
    "attn_head_size = 128 \n",
    "\n",
    "# Total number of transformer layers\n",
    "num_layers = 28\n",
    "\n",
    "# Sliding Window Attention\n",
    "max_seq_len = 64\n",
    "\n",
    "# Sampling temperature for softmax-based logit scaling\n",
    "temp = 0.7\n",
    "\n",
    "# Grouped Query Attention\n",
    "num_key_value_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327bac53-4fae-4e1e-bed9-402c5b3e05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beeb1a5-a506-4504-90b3-8260e82f9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df715698-9d3e-43e7-be8a-b5414c35ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize our KV cache for all transformer layers\n",
    "empty_kv = {}\n",
    "for i in range(num_layers):\n",
    "    # Shape of key and value tensors for each transformer layer\n",
    "    past_shape = (batch_size, num_key_value_heads, max_seq_len, attn_head_size)\n",
    "\n",
    "    # Initialize past keys for layer i (used in attention mechanism to avoid recomputation\n",
    "    empty_kv[f\"past_keys_{i}\"] = np.zeros(past_shape, dtype=np.float32)\n",
    "\n",
    "    # Initialize past values for layer i\n",
    "    empty_kv[f\"past_values_{i}\"] = np.zeros(past_shape, dtype=np.float32)\n",
    "\n",
    "len(empty_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a2af6-e1cb-496b-939d-36f0970c0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_kv.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f819a064-2d41-4f1d-b67c-6d596e2bdd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35fe260-822d-441f-a6d6-f30f49fefe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract 1 to get the index of the last token in the sequence (since indexing is 0-based)\n",
    "init_sequence_length = np.array(embedding_output.shape[1]-1, dtype=np.int32).reshape(1,1)\n",
    "\n",
    "# Set the maximum sequence length for the model's current forward pass\n",
    "max_seq_length = np.array([max_seq_len], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ed1e2-1c17-4306-bc76-c59f3720e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = {\n",
    "    \"past_seq_len\": init_sequence_length,\n",
    "    \"total_seq_len\": max_seq_length \n",
    "}\n",
    "seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d0e79-1f36-428c-aaca-e3698609aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72859658-e57f-4108-80b2-e127094f6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the inputs to expected size of seq_len of 64\n",
    "batch_size, seq_len, embed_dim = embedding_output.shape\n",
    "padding_id = 151643\n",
    "padded_embedding = np.full((batch_size, max_seq_length[0], embed_dim), padding_id, dtype=embedding_output.dtype) #np.zeros((batch_size, target_seq_len, embed_dim), dtype=embedding_output.dtype)\n",
    "\n",
    "padded_embedding[:, :seq_len, :] = embedding_output\n",
    "padded_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f0335-e7ac-480f-bb04-a8ec855d0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to ensure padding vectors were added\n",
    "padded_embedding[:,:seq_len+1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097c320-f1e7-4697-95d9-3cd38332aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_kv['past_keys_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd9908-20f5-4e5e-a18e-8c9a0e4f0341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_prompt_inputs = {\n",
    "    **empty_kv,\n",
    "    **seq_lens,\n",
    "    \"input_hidden_states\": padded_embedding,\n",
    "}\n",
    "init_prompt_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d21d8b-8970-41e4-8857-24955c842492",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c02aa8-b22a-4051-bee5-0c62e955258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_inputs['past_keys_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c68608-3661-4054-b48c-9a849c3b269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_outputs = ctx_session.run(None, init_prompt_inputs)\n",
    "len(prompt_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988ee3b-346d-4970-9168-e3dda4a65c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52344805-b174-47bb-a53a-3930dbef5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract final hidden states and present_keys/values\n",
    "print(\"Batch, prompt length (up to max 64 tokens), embedding size\")\n",
    "output_hidden_states = prompt_outputs[0]\n",
    "output_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a12c3-d415-40b0-bc98-a26a078ebad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Batch, key/value heads, prompt length (up to max 64 tokens), head dimension (size of projection for each head)\")\n",
    "print(\"Note: Total embedding size is 1536, this is split amongst 12 attention heads\")\n",
    "prompt_outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6475605-dfdc-478d-b7ce-012888b57e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_outputs[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078d99b-1116-491e-8a00-f58c0b72647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prompt Length x Head Dimension (Embedding Window)\")\n",
    "prompt_outputs[1][0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2f76c-a822-429e-8161-3032ff3be0f7",
   "metadata": {},
   "source": [
    "### To get longer initial context run ctx session over multiple prompts BUT use updated key/values after each prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d29e14-36b5-4eaf-91f3-b3f3c71229f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Populate initial past key/values\n",
    "# Must start with index==1 because index==0 is output_hidden_states (see genai_config.json)\n",
    "present_kv = {f\"past_keys_{i}\": prompt_outputs[1 + i * 2] for i in range(num_layers)}\n",
    "present_kv.update({f\"past_values_{i}\": prompt_outputs[1 + i * 2 + 1] for i in range(num_layers)})\n",
    "present_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8be8c-4027-4715-97ec-7b2d6839b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "present_kv.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226261b-3531-4415-bf83-63eb600c1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension checks\n",
    "present_kv[\"past_keys_0\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeba788-8418-480c-92d6-5fbac2f53d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "present_kv[\"past_keys_27\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23656c-777d-4faa-8278-5de48b300b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8109f-2f20-40b8-b213-a2ad260e399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = head_session.run(None, {\"output_hidden_states\": output_hidden_states})[0]\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239b2a9-d21b-4c05-aa6e-bb153544e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9020c8-3a7d-4165-8c75-5807bdb735a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d1e2c-68ba-40c0-8aea-b97dcba0e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_numpy(x: np.array, temperature: float=1) -> np.array:\n",
    "    # stabilize x in case of large numbers \n",
    "    x = x - np.max(x)\n",
    "\n",
    "    # Apply temperature\n",
    "    x = x/temperature\n",
    "\n",
    "    # Apply Softmax\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=-1)\n",
    "\n",
    "def top_k_probas(probas: np.array, k: int=5) -> np.array:\n",
    "    # Copy probas so in-place operations don't work on original variable\n",
    "    probas = probas.copy()\n",
    "    # Normalize probabilities\n",
    "    probas /= np.sum(probas)\n",
    "    # Using -probas to get in descending order\n",
    "    top_indices_sorted = np.argsort(-probas)[:k]\n",
    "    top_k_probas = probas[top_indices_sorted]\n",
    "\n",
    "    # Renormalize top-k probabilites to sum to 1 (probabilites must sum to 1 to use np.random.choice\n",
    "    top_k_probas /= np.sum(top_k_probas)\n",
    "\n",
    "    # Return top k probabilities\n",
    "    return top_indices_sorted, top_k_probas\n",
    "\n",
    "def apply_repetition_penalty(logits, generated_ids, penalty=1.1):\n",
    "    for token_id in set(generated_ids):\n",
    "        logits[token_id] /= penalty\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd92d8-81f6-4ea6-afa3-84f1346c7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax implemented\n",
    "# x-np.max(x) => for stability in case of large numbers\n",
    "softmax = lambda x, temperature=1: np.exp((x-np.max(x))/temperature)/np.sum(np.exp((x-np.max(x))/temperature), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b43fa-944c-49d3-b1f8-ac01a58c1835",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_numpy(logits[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357a6f4-a67c-4c80-bab2-1d220f43aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(logits[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490256c-2ff3-4f06-b81d-b3952e8ef041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabs last tokens logits\n",
    "temp = 0.6\n",
    "probas = softmax(logits[0,-1], temperature=temp)\n",
    "# probas = probas / probas.sum()\n",
    "next_token_id = int(np.random.choice(len(probas), p=probas)) #int(np.argmax(probas))\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4771b-8d3b-4fd7-8f5e-f8ba1c269e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afc517-ada4-4c86-bd21-5ed4cc075c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb01f20-ce67-4052-99a3-6c5787dc0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "present_kv.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45cece2-d6cc-4601-91ed-8055ad2cab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "present_kv['past_keys_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb5ba3-fae2-4dc8-8fc1-75ed6a96e604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# temp = 0.6\n",
    "start = time.time()\n",
    "max_tokens = 100\n",
    "top_k = 5\n",
    "generated_ids = [next_token_id]\n",
    "prev_seq_len = 64\n",
    "\n",
    "print(\"\\nInitial Query:\\n\", init_query)\n",
    "print(\"Generated:\")\n",
    "for _ in range(max_tokens):\n",
    "    input_ids = np.array([[next_token_id]], dtype=np.int64)\n",
    "    # print(tokenizer.decode(generated_ids, skip_special_tokens=True))\n",
    "    print(tokenizer.decode([next_token_id], skip_special_tokens=True),end=\"\")\n",
    "    embedding_output = embedding_session.run(None, {\"input_ids\": input_ids})[0]\n",
    "\n",
    "    # print(embedding_output.shape)\n",
    "\n",
    "    lengths = {\n",
    "    \"past_seq_len\": np.array([[prev_seq_len]], dtype=np.int32),\n",
    "    \"total_seq_len\": np.array([prev_seq_len + 1], dtype=np.int32)\n",
    "    }\n",
    "\n",
    "    iter_inputs = {\n",
    "    \"input_hidden_states\": embedding_output,\n",
    "    **present_kv,\n",
    "    **lengths,\n",
    "    }\n",
    "\n",
    "    iter_outputs = ctx_itr_session.run(None, iter_inputs)\n",
    "\n",
    "    # Hidden states are stored in last index of iter outputs\n",
    "    output_hidden_states = iter_outputs[0]\n",
    "\n",
    "    # For output tensor update key/value layers start at index = 0 \n",
    "    # NOTE: Remember output of ctx_itr_session has output_hidden_states at 0th index, start with 1\n",
    "    present_kv = {f\"past_keys_{i}\": iter_outputs[1 + i * 2] for i in range(num_layers)}\n",
    "    present_kv.update({f\"past_values_{i}\":iter_outputs[1 + i * 2 + 1] for i in range(num_layers)})\n",
    "    logits = head_session.run(None, {\"output_hidden_states\": output_hidden_states})[0]\n",
    "\n",
    "    token_logits = logits[0,-1]\n",
    "    token_logits = apply_repetition_penalty(token_logits, generated_ids, penalty=1.1)\n",
    "    # Get probabilities\n",
    "    probas = softmax(token_logits, temperature=temp)\n",
    "    top_indices, top_probas = top_k_probas(probas, k=top_k) \n",
    "    next_token_id = int(np.random.choice(top_indices, p=top_probas)) #int(np.argmax(probas))\n",
    "    generated_ids.append(next_token_id)\n",
    "    prev_seq_len += 1\n",
    "\n",
    "    if next_token_id == tokenizer.token_to_id(\"<｜end▁of▁sentence｜>\"):\n",
    "        break\n",
    "        \n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "tps = np.round((max_tokens / elapsed), 2)\n",
    "print(f\"\\nTokens Per Second: {tps}\")\n",
    "output_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88722a7-8dc7-40bb-a7f1-39f453f9895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "print(\"RAM Before Session Cleanup:\", psutil.virtual_memory().percent)\n",
    "print(\"Deleting Sessions .........\")\n",
    "del embedding_session\n",
    "del head_session\n",
    "del ctx_itr_session\n",
    "del ctx_session\n",
    "print(\"RAM After Session del before garbage collection:\", psutil.virtual_memory().percent)\n",
    "gc.collect\n",
    "print(\"RAM After Session Cleanup (session delete and gc):\", psutil.virtual_memory().percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2bcd6-b340-4c8e-865b-24c3ee261044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_arm64)",
   "language": "python",
   "name": "env_arm64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
