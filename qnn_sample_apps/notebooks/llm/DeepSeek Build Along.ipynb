{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18504196-b8a1-4f37-a5bc-d1165dfb7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "arch = platform.machine()\n",
    "sys = platform.system()\n",
    "processor = platform.processor()\n",
    "print(f\"{arch}\\n{sys}\\n{processor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3539a15-22cc-44fd-a461-8b88638b8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e6b41-df7b-42bc-8758-c9f4c5859304",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path.cwd().parent.parent\n",
    "onnx_root = Path(ort.__file__).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466dfca3-85f4-4557-9610-ebbd050ab322",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4d3cd-6bc1-4aea-b9d9-70010b2f1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d179adbf-79b1-4f9d-8e03-e1714361b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory where all .onnx dependencies are located\n",
    "model_subdirectory = \"qnn-deepseek-r1-distill-qwen-7b\"\n",
    "\n",
    "# The embeddings model is entry point, use netron to visualize\n",
    "model_name = \"deepseek_r1_7b_embeddings_quant_v1.0.onnx\"\n",
    "\n",
    "# This graph is used to process initial prompt, we can pass up to 64 tokens\n",
    "context_model = \"deepseek_r1_7b_ctx_v1.0.onnx_ctx.onnx\"\n",
    "\n",
    "# This graph is used to perform next word inference after the initial prompt\n",
    "context_model_iter = \"deepseek_r1_7b_iter_v1.0.onnx_ctx.onnx\"\n",
    "\n",
    "# This graph allows us to take hidden states and return logits\n",
    "head_model = \"deepseek_r1_7b_head_quant_v1.0.onnx\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer_json = \"tokenizer.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb04073-40a8-453c-b864-0b4e9b4b3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = root_dir/\"models\"/model_subdirectory/model_name\n",
    "ctx_path = root_dir/\"models\"/model_subdirectory/context_model\n",
    "ctx_path_itr = root_dir/\"models\"/model_subdirectory/context_model_iter\n",
    "head_path = root_dir/\"models\"/model_subdirectory/head_model\n",
    "tokenizer_path = root_dir/\"models\"/model_subdirectory/tokenizer_json\n",
    "hexagon_driver = onnx_root/\"capi\"/\"QnnHtp.dll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb88aca-78d6-412c-b2f5-5603b3f6956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hexagon_driver.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67445d4-7f91-433a-93f3-e6a82655f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_options = ort.SessionOptions()\n",
    "\n",
    "qnn_provider_options = {\n",
    "    # Path to the backend driver \"Hexagon\"\n",
    "    \"backend_path\": hexagon_driver,\n",
    "    # https://onnxruntime.ai/docs/execution-providers/QNN-ExecutionProvider.html#configuration-options\n",
    "    \"htp_performance_mode\": \"burst\",\n",
    "    \"soc_model\": \"60\",\n",
    "    # \"enable_htp_context_cache\": \"0\",\n",
    "    # \"profiling_level\": \"detailed\",\n",
    "    # \"profiling_file_path\": root_dir/\"models\"/model_subdirectory/\"profiling_deepseek_7b.csv\",\n",
    "    # Enabling graph optimization causes problems, need to look into this\n",
    "    \"htp_graph_finalization_optimization_mode\": \"3\",\n",
    "    \"qnn_context_priority\":\"high\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c04179-5cfb-4fe6-99d1-2f366e82f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_session = ort.InferenceSession(model_path,\n",
    "                                providers= [(\"QNNExecutionProvider\",qnn_provider_options)],\n",
    "                               sess_options= session_options\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d55e3-1eb0-4b84-b9f3-32f63671a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an inference session for the initial context graph\n",
    "ctx_session = ort.InferenceSession(ctx_path,\n",
    "                                    providers=[(\"QNNExecutionProvider\",qnn_provider_options)],\n",
    "                                    sess_options= session_options\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079d0a6-2568-49aa-8679-936ca202a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an inference session for the single prediction context graph (iter_ctx)\n",
    "ctx_itr_session = ort.InferenceSession(ctx_path_itr,\n",
    "                                         providers=[(\"QNNExecutionProvider\",qnn_provider_options)],\n",
    "                                         sess_options= session_options\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d14c11-563a-4bdc-a1fa-74d6dc5fbecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an inference session for the head session which will provide logits from hidden states\n",
    "head_session = ort.InferenceSession(head_path,\n",
    "                                providers= [(\"QNNExecutionProvider\",qnn_provider_options)],\n",
    "                               sess_options= session_options\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a195d3-3829-4d8e-94c4-d361a808a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_session.get_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a80a4-4308-484a-9fce-3cdccabe7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(str(tokenizer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a48361-3897-412b-9055-910ae7189b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"<｜User｜>\\nImagine you are a cyber security professional. Provide step by step reasons why AI models should be ran locally. Please consider all aspects of data privacy and cyber security\\n<｜Assistant｜><think>\\n\"\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36002e52-82f4-49da-8ce8-d8e165feb3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(query)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46696aa-a3d8-477f-a1c6-f1a1725e4a8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoding.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ee2bd-1c1e-49f5-91f3-71996b5b9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07400d-2e8e-4ae7-9eae-86bca1260734",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in embedding_session.get_inputs():\n",
    "    print(f\"Name: {layer.name}\")\n",
    "    print(f\"Shape: {layer.shape}\")\n",
    "    print(f\"Type: {layer.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa530a84-c604-41ea-8787-30d8d1a420d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = encoding.ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de968528-ca4c-459a-8eb4-0cf6181e750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = np.array([input_ids], dtype=np.int64)\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3307577-9f2b-4685-94d1-516546452a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output = embedding_session.run(None, {\"input_ids\": input_ids})[0]\n",
    "print(\"(batch, sequence length, embedding dimension)\")\n",
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf8536f-538b-4c25-9128-597c9b0abee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in embedding_session.get_outputs():\n",
    "    print(f\"Name: {layer.name}\")\n",
    "    print(f\"Shape: {layer.shape}\")\n",
    "    print(f\"Type: {layer.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32101737-583a-4f1f-a1cf-5d6bc8d1d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of input sequences processed simultaneously\n",
    "batch_size = 1\n",
    "\n",
    "# Current sequence length for initial prompt (number of tokens in current sequence)\n",
    "seq_len = embedding_output.shape[1]\n",
    "\n",
    "# Dimensionality of each token embedding vector\n",
    "hidden_size = embedding_output.shape[2]\n",
    "\n",
    "# Number of attention heads in each transformer layer\n",
    "num_heads = 28\n",
    "\n",
    "# Size of each attention head (should be hidden_size // num_heads\n",
    "attn_head_size = 128 \n",
    "\n",
    "# Total number of transformer layers\n",
    "num_layers = 28\n",
    "\n",
    "# SWA\n",
    "max_seq_len = 64\n",
    "\n",
    "# Number of key/value heads (key/value heads are shared amongst attention heads)\n",
    "num_key_value_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba3aab-6064-4ea5-83dd-c04591b5cf2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layers in ctx_session.get_inputs():\n",
    "    print(f\"Name: {layers.name}\")\n",
    "    print(f\"Shape: {layers.shape}\")\n",
    "    print(f\"Type: {layers.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea0494a-a1f5-45e0-91eb-7c3c541eb7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize our KV cache for all transformer layers\n",
    "empty_kv = {}\n",
    "for i in range(num_layers):\n",
    "    # Shape of key and value tensors for each transformer layer\n",
    "    past_shape = (batch_size, num_key_value_heads, max_seq_len, attn_head_size)\n",
    "\n",
    "    # Initialize past keys for layer i (used in attention mechanism to avoid recomputation\n",
    "    empty_kv[f\"past_keys_{i}\"] = np.zeros(past_shape, dtype=np.float32)\n",
    "\n",
    "    # Initialize past values for layer i\n",
    "    empty_kv[f\"past_values_{i}\"] = np.zeros(past_shape, dtype=np.float32)\n",
    "\n",
    "len(empty_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7443b-f2b6-4252-be42-61cbc42abdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_kv.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c1eeb-2353-45e3-a001-b1184b8eeab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract 1 to get the index of the last token in the sequence (since indexing is 0-based)\n",
    "init_sequence_length = np.array(embedding_output.shape[1]-1, dtype=np.int32).reshape(1,1)\n",
    "\n",
    "# Set the maximum sequence length for the model's current forward pass\n",
    "max_seq_length = np.array([max_seq_len], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1829726-be1d-4979-9fd4-60ff29a4512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = {\n",
    "    \"past_seq_len\": init_sequence_length,\n",
    "    \"total_seq_len\": max_seq_length \n",
    "}\n",
    "seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a57bb-1863-4c19-a093-7cd43c43401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_sequence_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20d051-c553-4072-a142-aa0a40e3d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155070f-9eae-4eaf-bdcb-4d9d0104aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.token_to_id(\"<｜end▁of▁sentence｜>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27553f9c-9a6c-4cf9-b48c-05d4b5714b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the inputs to expected size of prefill graph\n",
    "batch_size, seq_len, embed_dim = embedding_output.shape\n",
    "padding_id = 151643\n",
    "padded_embedding = np.full((batch_size, max_seq_length[0], embed_dim), padding_id, dtype=embedding_output.dtype)\n",
    "padded_embedding[:, :seq_len, :] = embedding_output \n",
    "padded_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533926d2-4d6e-4ab7-8412-d046468d493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_embedding[:, :seq_len+1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85419e50-cbb7-4d42-b900-31ee974c6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96188df-fa43-43ca-8a15-e8af4ea9a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefill_inputs = {\n",
    "    **empty_kv,\n",
    "    **seq_lens,\n",
    "    \"input_hidden_states\": padded_embedding,\n",
    "}\n",
    "prefill_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa41d47-361f-4e24-8535-4eedd4bff5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_outputs = ctx_session.run(None, prefill_inputs)\n",
    "len(prompt_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145fd96-38b3-400a-bc4e-8ae7ae137ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in ctx_session.get_outputs():\n",
    "    print(f\"Name: {layer.name}\")\n",
    "    print(f\"Shape: {layer.shape}\")\n",
    "    print(f\"Type: {layer.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec63d5-6627-4bbb-8f05-902626d0f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Batch, key/value heads, prompt length, head dimension (size of projection for each head)\")\n",
    "prompt_outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf96ec7-bf61-46af-b099-ddce6f678213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract final hidden states and present_keys/values\n",
    "print(\"Batch, prompt length, vector embedding size\")\n",
    "output_hidden_states = prompt_outputs[0]\n",
    "output_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f0575-0709-451a-acbb-51a90a26be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_outputs[1 + 0 * 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b613a7c-9e30-435d-b53f-187c081633a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update KV cache from Prefill calculation\n",
    "# Must start with index==1 because index==0 is output_hidden_states\n",
    "present_kv = {f\"past_keys_{i}\": prompt_outputs[1 + i * 2] for i in range(num_layers)}\n",
    "present_kv.update({f\"past_values_{i}\": prompt_outputs[1 + i * 2 + 1] for i in range(num_layers)})\n",
    "present_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92afd4-41b9-45a3-9a9a-2c939af0a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in head_session.get_inputs():\n",
    "    print(f\"Name: {layer.name}\")\n",
    "    print(f\"Shape: {layer.shape}\")\n",
    "    print(f\"Type: {layer.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ae4d0-6667-4a78-aa82-bfd43807496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = head_session.run(None, {\"output_hidden_states\": output_hidden_states})[0]\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510bd7f-8ddc-4635-aa66-2090cc28e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in head_session.get_outputs():\n",
    "    print(f\"Name: {layer.name}\")\n",
    "    print(f\"Shape: {layer.shape}\")\n",
    "    print(f\"Type: {layer.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b6752-e10a-4f6a-abc7-9400e20c674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Batch Size, Sequence Length, Vocabulary Size\")\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e54429-7404-4a98-ac5d-8b877c10d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_numpy(x: np.array, temperature: float=1) -> np.array:\n",
    "    # stabilize x in case of large numbers \n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "\n",
    "    # Apply temperature\n",
    "    x = x/temperature\n",
    "\n",
    "    # Apply Softmax\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfb721-289b-4ad1-8bb4-b3619d7be7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_probas(probas: np.array, k: int=5) -> np.array:\n",
    "    # Copy probas so in-place operations don't work on original variable\n",
    "    probas = probas.copy()\n",
    "    # Normalize probabilities\n",
    "    probas /= np.sum(probas)\n",
    "    # Using -probas to get in descending order\n",
    "    top_indices_sorted = np.argsort(-probas)[:k]\n",
    "    top_k_probas = probas[top_indices_sorted]\n",
    "\n",
    "    # Renormalize top-k probabilites to sum to 1 (probabilites must sum to 1 to use np.random.choice\n",
    "    top_k_probas /= np.sum(top_k_probas)\n",
    "\n",
    "    # Return top k probabilities\n",
    "    return top_indices_sorted, top_k_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb0273-7ad1-4eff-b2a6-78de8e245347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_repetition_penalty(logits, generated_ids, penalty=1.1):\n",
    "    for token_id in set(generated_ids):\n",
    "        logits[token_id] /= penalty\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdcfcb-05ae-424e-b4aa-32a70b09430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = lambda x, temperature=1: np.exp((x-np.max(x, axis=-1, keepdims=True))/temperature)/np.sum(np.exp((x-np.max(x, axis=-1, keepdims=True))/temperature), axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8217b1d5-1829-4d16-a373-0ce91c5d96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling temperature for softmax-based logit scaling\n",
    "temp = 0.7\n",
    "probas = softmax(logits[0,-1], temperature=temp)\n",
    "\n",
    "next_token_id = int(np.random.choice(len(probas), p=probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05404898-44e9-4c16-9ebe-ec83ac8c25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd1b31-48fb-48be-82e1-2dd3021b32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in ctx_itr_session.get_inputs():\n",
    "    print(f\"Name: {layer.name}\")\n",
    "    print(f\"Shape: {layer.shape}\")\n",
    "    print(f\"Type: {layer.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67e890-10a4-455b-8d23-831f047da2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "max_tokens = 100\n",
    "top_k = 5\n",
    "generated_ids = [next_token_id]\n",
    "prev_seq_len = 64\n",
    "\n",
    "print(\"\\nInitial Query:\\n\", query)\n",
    "print(\"Generated:\")\n",
    "\n",
    "for _ in range(max_tokens):\n",
    "    input_ids = np.array([[next_token_id]], dtype=np.int64)\n",
    "    print(tokenizer.decode([next_token_id], skip_special_tokens=True),end=\"\")\n",
    "    embedding_output = embedding_session.run(None, {\"input_ids\": input_ids})[0]\n",
    "    \n",
    "    lengths = {\n",
    "    \"past_seq_len\": np.array([[prev_seq_len]], dtype=np.int32),\n",
    "    \"total_seq_len\": np.array([prev_seq_len + 1], dtype=np.int32)\n",
    "    }\n",
    "\n",
    "    iter_inputs = {\n",
    "    \"input_hidden_states\": embedding_output,\n",
    "    **present_kv,\n",
    "    **lengths,\n",
    "    }\n",
    "\n",
    "    iter_outputs = ctx_itr_session.run(None, iter_inputs)\n",
    "\n",
    "    output_hidden_states = iter_outputs[0]\n",
    "\n",
    "    present_kv = {f\"past_keys_{i}\": iter_outputs[1 + i * 2] for i in range(num_layers)}\n",
    "    present_kv.update({f\"past_values_{i}\":iter_outputs[1 + i * 2 + 1] for i in range(num_layers)})\n",
    "    \n",
    "    logits = head_session.run(None, {\"output_hidden_states\": output_hidden_states})[0]    \n",
    "\n",
    "    token_logits = logits[0,-1]\n",
    "    token_logits = apply_repetition_penalty(token_logits, generated_ids, penalty=1.1)\n",
    "    # Get probabilities\n",
    "    probas = softmax(token_logits, temperature=temp)\n",
    "    top_indices, top_probas = top_k_probas(probas, k=top_k) \n",
    "    next_token_id = int(np.random.choice(top_indices, p=top_probas)) #int(np.argmax(probas))\n",
    "    generated_ids.append(next_token_id)\n",
    "    prev_seq_len += 1\n",
    "\n",
    "    if next_token_id == tokenizer.token_to_id(\"<｜end▁of▁sentence｜>\"):\n",
    "        break\n",
    "        \n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "tps = np.round((max_tokens / elapsed), 2)\n",
    "print(f\"\\nTokens Per Second: {tps}\")\n",
    "output_text = tokenizer.decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb29d83-9b95-40d5-87d6-327a5b3d5531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_arm64)",
   "language": "python",
   "name": "env_arm64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
