{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede68370-32c3-4009-bc2b-2502bce0a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c152d4-00b0-4946-8202-698f8bf25322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check to ensure we are in the proper environment, remember we need to be in pure arm64\n",
    "import platform\n",
    "\n",
    "arch = platform.machine()\n",
    "sys = platform.system()\n",
    "processor = platform.processor()\n",
    "print(f\"{arch}\\n{sys}\\n{processor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d0e3f-d3ef-433c-a134-a9c7c3ada10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary tools that we need\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c2eb2-0e89-42cd-b112-7ea0b2a0faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the root directory as a reference\n",
    "root_dir = Path.cwd().parent.parent\n",
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b0ac5-3a4f-46c5-b14a-f5bafed7b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the path to onnxruntime therefore we can grab hexagon driver\n",
    "onnx_root = Path(ort.__file__).parent\n",
    "onnx_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028612de-d6a6-4d6a-a30a-5f2f8c978548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory where all .onnx dependencies are located\n",
    "model_subdirectory = \"cpu-deepseek-r1-distill-qwen-7b\"\n",
    "\n",
    "# The embeddings model is entry point, use netron to visualize\n",
    "model_name = \"deepseek-r1-distill-qwen-7b-cpu-int4-rtn-block-32-acc-level-4.onnx\"\n",
    "\n",
    "# Genai configuration path\n",
    "configuration_json = \"genai_config.json\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer_json = \"tokenizer.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a4721-e62c-48c2-962b-650738f9d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solidifying all paths\n",
    "\n",
    "model_path = root_dir/\"models\"/model_subdirectory/model_name\n",
    "tokenizer_path = root_dir/\"models\"/model_subdirectory/tokenizer_json\n",
    "config_path = root_dir/\"models\"/model_subdirectory/configuration_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a804b38-f006-468d-9052-f710f90fc90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383718b-aa81-4897-9372-9c26519f353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_options = ort.SessionOptions()\n",
    "\n",
    "# Creating an inference session for the embedding graph\n",
    "session = ort.InferenceSession(model_path)\n",
    "\n",
    "session.get_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff6b9fe-5d0f-48fd-8e9c-25b8d8b246fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = session.get_inputs()\n",
    "outputs = session.get_outputs()\n",
    "input_0 = inputs[0]\n",
    "output_0 = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ccb3e9-2ec0-42b9-8495-f295483ead0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected Input Shape: {input_0.shape}\")\n",
    "print(f\"Expected Input Type: {input_0.type}\")\n",
    "print(f\"Expected Input Name: {input_0.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf9daa-63f6-41c4-9f6e-6351131aa27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected Output Shape: {output_0.shape}\")\n",
    "print(f\"Expected Output Type: {output_0.type}\")\n",
    "print(f\"Expected Output Name: {output_0.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be61fe-42a0-482a-91ac-26c53b9ccfcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in inputs:\n",
    "    print(f\"Name: {layer.name}\\n\\tExpected Input Shape: {layer.shape}\\n\\tExpected Input Type: {layer.type}\")\n",
    "    print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb915bc4-fe7d-4b9d-a997-a17bff65a635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in outputs:\n",
    "    print(f\"Name: {layer.name}\\n\\tExpected Input Shape: {layer.shape}\\n\\tExpected Input Type: {layer.type}\")\n",
    "    print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79c33e-967a-4652-af39-e6afa06a0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in tokenizer using tokenizer path above\n",
    "tokenizer = Tokenizer.from_file(str(tokenizer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3f10c-ac54-408e-b77b-cf93dc4b794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_bank = {\"security\":\"<｜User｜>\\nImagine you are a cyber security professional. Provide step by step reasons why AI models should be ran locally. Please consider all aspects of data privacy and cyber security\\n<｜Assistant｜><think>\\n\",\n",
    "              \"cooking\":\"<｜User｜>\\nGive me a step-by-step baked chicken recipe, including ingredients, cook time, and sides.\\n<｜Assistant｜>\\n\",\n",
    "              \"therapist\":\"<｜User｜>\\nImagine you are a therapist with a background in cyber security. I'm am currently very anxious about my data being stolen\\\n",
    "              can you provide me remedies to help with my depression and anxiety\\n<｜Assistant｜><think>\\n\"\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5359713-ee6e-430c-9e29-a06227bff28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An initial query\n",
    "init_query = \"<｜User｜>\\nYou are an expert computer scientist. Why does running AI models on NPU perform better than on CPU?\\n<｜Assistant｜><think>\\n\"\n",
    "encoding = tokenizer.encode(init_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e20101-0a5b-490d-bb4a-a25e02617b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token IDs:\", encoding.ids)\n",
    "print(\"Tokens:\", encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad39ea5-7780-428b-b29c-8b1cceeb5138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = encoding.ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a6da0-a753-4f53-ae38-0775a3d27ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing inputs for prompt\n",
    "\n",
    "# Number of input sequences processed simultaneously\n",
    "batch_size = 1\n",
    "\n",
    "# Current sequence length for initial prompt (number of tokens in current sequence)\n",
    "seq_len = len(input_ids)#.shape[2]\n",
    "\n",
    "# Dimensionality of each token embedding vector\n",
    "# hidden_size = embedding_output.shape[2]\n",
    "\n",
    "# Number of attention heads in each transformer layer\n",
    "num_heads = 28\n",
    "\n",
    "# Size of each attention head (should be hidden_size // num_heads\n",
    "attn_head_size = 128 #hidden_size//num_heads # ex. 1536/12 = 128\n",
    "\n",
    "# Total number of transformer layers\n",
    "num_layers = 28\n",
    "\n",
    "# This is not the model's global context window (131072), this is the max number of tokens passed in the first forward pass\n",
    "max_seq_len = len(input_ids)\n",
    "\n",
    "# Sampling temperature for softmax-based logit scaling\n",
    "temp = 0.9\n",
    "\n",
    "# Number of key/value heads (key/value heads are shared amongst attention heads)\n",
    "num_key_value_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc73c0-0c90-46e1-8211-88b1ea17f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b5ac0-fd4e-4dae-a8ab-a1b7acb54410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad the inputs to expected size of seq_len of 64\n",
    "# target_seq_len = 64\n",
    "# input_ids += [pad_token_id] * (target_seq_len - len(input_ids))\n",
    "input_ids = np.array([input_ids], dtype=np.int64)\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92643dde-6162-4ec7-80ab-1bf944783f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.ones((batch_size, max_seq_len), dtype=np.int64)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab86c0a-5d77-4211-922d-f668830bacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac1d58-414f-4a36-be94-f8d346662120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize our KV cache for all transformer layers\n",
    "empty_kv = {}\n",
    "for i in range(num_layers):\n",
    "    # Shape of key and value tensors for each transformer layer\n",
    "    past_shape = (batch_size, num_key_value_heads, max_seq_len, attn_head_size)\n",
    "\n",
    "    # Initialize past keys for layer i (used in attention mechanism to avoid recomputation\n",
    "    empty_kv[f\"past_key_values.{i}.key\"] = np.zeros(past_shape, dtype=np.float32)\n",
    "\n",
    "    # Initialize past values for layer i\n",
    "    empty_kv[f\"past_key_values.{i}.value\"] = np.zeros(past_shape, dtype=np.float32)\n",
    "\n",
    "len(empty_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a2b43c-43b5-44d8-a151-79558439a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_kv.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e210d2-c650-4df4-9a53-8978f6626fee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_prompt_inputs = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\":attention_mask,\n",
    "    **empty_kv,\n",
    "}\n",
    "init_prompt_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e2009-109d-4a91-a56e-26bcc232333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_inputs.get(\"past_key_values.0.key\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4dd5e7-9e36-481f-8a6f-332aae0c1721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run embedding session first\n",
    "session_output = session.run(None, init_prompt_inputs)\n",
    "# print(\"Logits:\\n(batch, sequence length, vocab size)\")\n",
    "session_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a18ab-7a2c-4ba6-8649-8915e416a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logits:\\n(batch, sequence length, vocab size)\")\n",
    "session_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaca336-9fe7-4495-a7e6-cc27f5c722f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KV Cache:\\n(batch, num_kv_heads, sequence length, attn_head_size)\")\n",
    "session_output[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2f76c-a822-429e-8161-3032ff3be0f7",
   "metadata": {},
   "source": [
    "### To get longer initial context run ctx session over multiple prompts BUT use updated key/values after each prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d29e14-36b5-4eaf-91f3-b3f3c71229f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update kv cache\n",
    "present_kv = {f\"past_key_values.{i}.key\": session_output[1 + i * 2] for i in range(num_layers)}\n",
    "present_kv.update({f\"past_key_values.{i}.value\": session_output[1 + i * 2 + 1] for i in range(num_layers)})\n",
    "present_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8be8c-4027-4715-97ec-7b2d6839b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "present_kv.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226261b-3531-4415-bf83-63eb600c1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension checks\n",
    "present_kv[\"past_key_values.0.key\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeba788-8418-480c-92d6-5fbac2f53d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "present_kv[\"past_key_values.27.value\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239b2a9-d21b-4c05-aa6e-bb153544e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = session_output[0]\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9020c8-3a7d-4165-8c75-5807bdb735a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d1e2c-68ba-40c0-8aea-b97dcba0e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_numpy(x: np.array, temperature: float=1) -> np.array:\n",
    "    # stabilize x in case of large numbers \n",
    "    x = x - np.max(x)\n",
    "\n",
    "    # Apply temperature\n",
    "    x = x/temperature\n",
    "\n",
    "    # Apply Softmax\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=-1)\n",
    "\n",
    "def top_k_probas(probas: np.array, k: int=5) -> np.array:\n",
    "    # Copy probas so in-place operations don't work on original variable\n",
    "    probas = probas.copy()\n",
    "    # Normalize probabilities\n",
    "    probas /= np.sum(probas)\n",
    "    # Using -probas to get in descending order\n",
    "    top_indices_sorted = np.argsort(-probas)[:k]\n",
    "    top_k_probas = probas[top_indices_sorted]\n",
    "\n",
    "    # Renormalize top-k probabilites to sum to 1 (probabilites must sum to 1 to use np.random.choice\n",
    "    top_k_probas /= np.sum(top_k_probas)\n",
    "\n",
    "    # Return top k probabilities\n",
    "    return top_indices_sorted, top_k_probas\n",
    "\n",
    "def apply_repetition_penalty(logits, generated_ids, penalty=1.1):\n",
    "    for token_id in set(generated_ids):\n",
    "        logits[token_id] /= penalty\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd92d8-81f6-4ea6-afa3-84f1346c7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax implemented\n",
    "# x-np.max(x) => for stability in case of large numbers\n",
    "softmax = lambda x, temperature=1: np.exp((x-np.max(x))/temperature)/np.sum(np.exp((x-np.max(x))/temperature), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b43fa-944c-49d3-b1f8-ac01a58c1835",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_numpy(logits[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357a6f4-a67c-4c80-bab2-1d220f43aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(logits[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490256c-2ff3-4f06-b81d-b3952e8ef041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabs last tokens logits\n",
    "temp = 0.6\n",
    "probas = softmax(logits[0,-1], temperature=temp)\n",
    "# probas = probas / probas.sum()\n",
    "next_token_id = int(np.random.choice(len(probas), p=probas)) #int(np.argmax(probas))\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4771b-8d3b-4fd7-8f5e-f8ba1c269e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afc517-ada4-4c86-bd21-5ed4cc075c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b35f9-5085-4270-8ead-57e2108dd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb5ba3-fae2-4dc8-8fc1-75ed6a96e604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# temp = 0.6\n",
    "start = time.time()\n",
    "max_tokens = 1000\n",
    "top_k = 5\n",
    "generated_ids = [next_token_id]\n",
    "prev_seq_len = logits.shape[1]\n",
    "# print(prev_seq_len)\n",
    "# print(attention_mask.shape)\n",
    "print(\"\\nInitial Query:\\n\", init_query)\n",
    "print(\"Generated:\")\n",
    "for _ in range(max_tokens):\n",
    "    input_ids = np.array([[next_token_id]], dtype=np.int64)\n",
    "    # print(tokenizer.decode(generated_ids, skip_special_tokens=True))\n",
    "    print(tokenizer.decode([next_token_id], skip_special_tokens=True),end=\"\")\n",
    "    \n",
    "    iter_inputs = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "    **present_kv,\n",
    "    }\n",
    "\n",
    "    session_output = session.run(None, iter_inputs)\n",
    "    prev_seq_len += 1\n",
    "    # Update attention mask\n",
    "    attention_mask = np.ones((batch_size, prev_seq_len), dtype=np.int64)\n",
    "    # Update KV Cache\n",
    "    present_kv = {f\"past_key_values.{i}.key\": session_output[1 + i * 2] for i in range(num_layers)}\n",
    "    present_kv.update({f\"past_key_values.{i}.value\": session_output[1 + i * 2 + 1] for i in range(num_layers)})\n",
    "    # print(prev_seq_len)\n",
    "    # print(present_kv.get(\"past_key_values.0.key\").shape)\n",
    "    # print(len(attention_mask))\n",
    "    logits = session_output[0]\n",
    "\n",
    "    token_logits = logits[0,-1]\n",
    "    token_logits = apply_repetition_penalty(token_logits, generated_ids, penalty=1.1)\n",
    "#     # Get probabilities\n",
    "    probas = softmax(token_logits, temperature=temp)\n",
    "    top_indices, top_probas = top_k_probas(probas, k=top_k) \n",
    "    next_token_id = int(np.random.choice(top_indices, p=top_probas)) #int(np.argmax(probas))\n",
    "    generated_ids.append(next_token_id)\n",
    "\n",
    "\n",
    "    if next_token_id == tokenizer.token_to_id(\"< | end_of_sentence | >\"):\n",
    "        break\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "tps = np.round((max_tokens / elapsed), 2)\n",
    "print(f\"\\nTokens Per Second: {tps}\")\n",
    "output_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8eb4ef-ec9c-4278-b442-dbe34eb08081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
